---
layout: post
title: CMA-ES - From Preliminaries to Source Code
tags: [Evolutionary Optimization, Evolutionary Algorithms, Evolutionary Strategies, CMA-ES, Genetic Algorithms, Swarm Optimization]
---

<style>
body {
text-align: justify}
</style>

## [1] Motivation
Optimization is an important concept in mathematics and related fields. The idea is to maximize (or minimize) a certain desired criteria (function, specifically called objective function or goal function) while satisfying the relevant restrictions (called constraints), if there are any. Optimization is common in traditional engineering industries, decision making, operations research, machine/deep learning, artificial intelligence and many more, making it almost ubiquitous. 

For complex optimization problems, the global landscape of the objective function is rugged, multi-modal and patchy, where feasible regions are separated by unfeasible regions in the design variable space. The landscape becomes more complex as the dimensionality of the problem increases. Gradient based methods terminate at local minimum and unless the objective function is convex (for a minimization problem), will therefore converge to a local optimum. In problems with complex objective function landscape, we therefore need gradient free search algorithms that possess a global search capability.  

Optimization problems where one can numerically obtains the influence of the affecting (decision) variables on the value of the objective function, even in the absence of some analytic algebraic model of the system, are well suited to be solved through gradient free population based optimization metaheuristic techniques, popularly known as *evolutionary algorithms*. The search procedure of these algorithms uses mechanisms inspired by biological evolution, such as reproduction, mutation, recombination and selection. One begins with an initial pool of guess solutions. This pool of solutions is referred as *population*  and the *population size* indicates the number of candidate solutions in the population. Based on the algorithmic rules, the population members explore the search space and evolve through *generations* by exchanging information between fellow members. With an appropriate implementation, one can expect the population members to converge towards the region where the mean value of the objective function is low. Convergence of the population towards an extremely low mean objective function value suggests that the algorithm approaches or approximates the global optimum. 

There are several classes of evolution(or related)-inspired search algorithms: generic algorithms, genetic programming, swarm algorithms, ant-colony optimization, evolutionary strategies etc., and numerous variants of these. The choice of the algorithms is often based on the complexity of the problem. No single algorithm is usually capable to solve all types of problems. That being said evolutionary strategies represent the current state-of-the-art algorithms. The most impressive algorithm of this class is the *covariance matrix adaptation evolutionary strategy* or **CMA-ES**. This tutorial is aimed at presenting a detailed understanding of CMA-ES to the reader. The attempt is to begin from the very basic building blocks of the algorithm all the way to the source code in Python.

## [2] Preliminaries
We begin with some preliminary concepts that prove useful for later on purposes. We will first have some high level discussion of the eigen decomposition of a square matrix and discuss some key concepts of a multivariate normal distribution. 

###[2.1] Eigendecomposition of a square matrix
Eigenvectors and eigenvalues are numbers and vectors associated to **square matrices**, and together they provide the eigendecomposition of a matrix. Eigendecomposition does not exists for all the square matrices, it has a convenient expression for a class of matrices that are very frequently used in mutivariate analysis: correlation, covariance etc. matrices. A vectors **u** is an eigenvector of a square matrix **A** if: 

**Au** = $\lambda$**u**

A neat way of putting this is as follows: vector **u** is an eigenvector of **A** if upon multiplication with **A**, it is **unrotated**. Strictly speaking, there is an infinity of eigenvectors associated with an eigenvalue of a matrix. Thus, for most applications, we normalize the eigenvectors. Enough said. Traditionally, we put together the set of eigenvectors of **A** in a matrix denoted by **U**. Each column of **U** is an eigenvector of **A**. The eigenvalues are stored in a diagonal matrix denoted by $\Lambda$, where the diagonal elements gives the eigenvalues.   

**AU** = **U**$\lambda$

###[2.2] Eigendecomposition of a (semi-)definite matrices.

